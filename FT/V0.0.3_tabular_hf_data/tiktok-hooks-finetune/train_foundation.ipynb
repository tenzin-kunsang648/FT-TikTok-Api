{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers datasets accelerate scikit-learn torch pandas numpy -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSequenceClassification,\n",
        "    TrainingArguments, Trainer, EarlyStoppingCallback\n",
        ")\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import torch\n",
        "import json\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "BASE_MODEL = 'distilbert-base-uncased'\n",
        "MAX_LENGTH = 256\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_EPOCHS = 3\n",
        "TASK = 'regression'\n",
        "OUTPUT_DIR = \"/content/models_foundation\"\n",
        "\n",
        "print(f\"Model: {BASE_MODEL}, Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds = load_dataset(\"benxh/tiktok-hooks-finetune\")\n",
        "df = ds['train'].to_pandas()\n",
        "\n",
        "df['engagement_rate'] = (df['likes'] + df['comments'] + df['shares']) / (df['views'] + 1)\n",
        "df['virality_score'] = df['engagement_rate'] * np.log1p(df['views'])\n",
        "df['text_input'] = df['text_hook'].astype(str) + \" [SEP] \" + df['caption'].astype(str)\n",
        "df = df.dropna(subset=['text_input', 'virality_score'])\n",
        "\n",
        "df['uploaded_at'] = pd.to_datetime(df['uploaded_at'])\n",
        "df = df.sort_values('uploaded_at')\n",
        "\n",
        "train_size = int(len(df) * 0.70)\n",
        "val_size = int(len(df) * 0.15)\n",
        "\n",
        "train_df = df.iloc[:train_size].copy()\n",
        "val_df = df.iloc[train_size:train_size+val_size].copy()\n",
        "test_df = df.iloc[train_size+val_size:].copy()\n",
        "\n",
        "print(f\"Train: {len(train_df):,}, Val: {len(val_df):,}, Test: {len(test_df):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text_input'], truncation=True, padding='max_length', max_length=MAX_LENGTH)\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df[['text_input', 'virality_score']])\n",
        "val_dataset = Dataset.from_pandas(val_df[['text_input', 'virality_score']])\n",
        "test_dataset = Dataset.from_pandas(test_df[['text_input', 'virality_score']])\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "train_dataset = train_dataset.map(lambda x: {'labels': float(x['virality_score'])}, remove_columns=['virality_score'])\n",
        "val_dataset = val_dataset.map(lambda x: {'labels': float(x['virality_score'])}, remove_columns=['virality_score'])\n",
        "test_dataset = test_dataset.map(lambda x: {'labels': float(x['virality_score'])}, remove_columns=['virality_score'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=1, problem_type=\"regression\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    mae = mean_absolute_error(labels, predictions)\n",
        "    rmse = np.sqrt(mean_squared_error(labels, predictions))\n",
        "    r2 = r2_score(labels, predictions)\n",
        "    return {'mae': mae, 'rmse': rmse, 'r2': r2}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "output_dir = f\"{OUTPUT_DIR}/{BASE_MODEL.replace('/', '_')}_{TASK}_{timestamp}\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=100,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"mae\",\n",
        "    greater_is_better=False,\n",
        "    save_total_limit=2,\n",
        "    seed=42,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_results = trainer.evaluate(test_dataset)\n",
        "print(\"Test Results:\")\n",
        "for key, value in test_results.items():\n",
        "    if 'eval_' in key:\n",
        "        print(f\"  {key}: {value:.4f}\")\n",
        "\n",
        "trainer.save_model()\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "metadata = {\n",
        "    'base_model': BASE_MODEL,\n",
        "    'task': TASK,\n",
        "    'test_results': {k: float(v) for k, v in test_results.items()},\n",
        "    'timestamp': timestamp,\n",
        "}\n",
        "\n",
        "with open(f'{output_dir}/metadata.json', 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(f\"Model saved to {output_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "model_dir = \"/content/models_foundation\"\n",
        "if os.path.exists(model_dir):\n",
        "    model_folders = [os.path.join(model_dir, d) for d in os.listdir(model_dir) \n",
        "                     if os.path.isdir(os.path.join(model_dir, d))]\n",
        "    if model_folders:\n",
        "        latest_model = max(model_folders, key=os.path.getmtime)\n",
        "        for file in ['pytorch_model.bin', 'config.json', 'tokenizer_config.json', 'vocab.txt', 'metadata.json']:\n",
        "            file_path = os.path.join(latest_model, file)\n",
        "            if os.path.exists(file_path):\n",
        "                files.download(file_path)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
